# Story lungs: eXplainable predictions for post operational risks {#story-lungs}

*Authors: Maciej Bartczak (UW), Marika Partyka (PW)*

*Mentors: Aleksandra Radziwiłł (McKinsey & Company), Maciej Krasowski (McKinsey & Company)*




## Introduction 
 Science allows us to understand the world better. New technologies, data collection solves the problems not only of large companies but also of ordinary people. Especially if human life is at stake.
 They say that cancer is the killer of the 21st century. That's why even small attempts to subdue this problem are important. 
 In our work, we deal with lung cancer. We try to predict the chances of survival of a patient who has had a tumor removal surgery. Note that we do not generally anticipate the chances of survival here, but only consider a particular group of patients who have cancer and have been qualified for surgery. Therefore, along the way we may encounter many non-intuitive conclusions, we may encounter here the survivorship bias. That is why the role of explaining the model in this case is so important, as we will show in the next parts of this chapter. But let's focus on the model for a moment.


## Model 

The dataset consists of the following varaibles.

Numerical

Variable | Unit
--- | ---
`date_birth` | date
`date_start_treatment` | date
`date_surgery` | date
`tumor_size_x` | cm
`tumor_size_y` | cm
`tumor_size_z` | cm
`years_smoking` | years
`age` | years
`time_to_surgery` | years ("today" - date_surgery)

Categorical

Variable | Decription | Values
:--- | :--- | :---
`sex` | subject sex | male/female
`histopatological_diagnosis` | type of cancer | Rak płaskonabłonkowy, pleomorficzny, ...*
`symptoms` | whether symptoms were observed | yes/no
`lung_cancer_in_family` | whether family member had cancer | yes/no
`stadium_uicc` | severity of tumor | IA1, IA2, IA3, IB, IIA, IIB, IIIA, IIIB, IVA, IVB*
`alive` | whether subject is alive | yes/no (**target variable**)

About 1/3 af values of varaibles annotated with * was missing.

Survivability was registered `time_to_surgery` years after the procedure, for the majority of the subjects not later than 1 year after the procedure, and at most after 12 years.

We have tried out several models as well as different preprocessing strategies. However, all of the approaches yielded similar results differing no more than 0.01 of cross validation acuracy and 0.01 ROC AUC score. These are the models that were utilized:

- Logistic regression
- Logistic regression with hyperparameters cross validation
- Random Forest
- XGBoost
- Neural Net - 10 hidden neurons
- Neural Net - 30 hidden neurons
- Neural Net - 2 x 10 hidden neurons

As we have indentified higle defendent features as well as observed that tumor sizes disturbed the explaination we have employed following preprocessing strategies:

- encode and normalize
- encode, remove highly dependent features and normalize 
- encode, introduce tumor volume, discard tumor sizes, remove highly dependent features and normalize 

Finally we have settled on Neural Net with 10 hidden neurons with following receiver operating curve.

```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE}
knitr::include_graphics('images/07_auc_nn.png')
```


## Explanations
The final model we based on was logistic regression. 

We are based on 3 methods of explaining models, mainly Ceteris Paribus as well as Shap and Variable Importance. Let's start by explaining at dataset level. Let's see how Variable Importance behaves.

```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE, fig.cap='Variable Importance for Logistic Regression model. Under the date_surgery variable is the number of years that have elapsed between that date and the end of the study. As the explanation shows that this variable is the most important, we will look at it in more detail later in this chapter.'}
knitr::include_graphics('images/Variable_imp_07.png')
```

The second most important is the UICC stage. This variable tells us how advanced the cancer we cut out is. You can guess that the more advanced the stage, the bigger and harder the tumor is to cut out. We may wonder why the third most important variable is date of birth, not age. In fact, these two variables are obviously very correlated and the above results are the result of the distribution of importance between the two variables. Interestingly, the period of time the patient smoked cigarettes does not affect the outcome too much. Of course, if we were to consider the chances of getting lung cancer, or overall survival, this variable could be much more important. However, let's remember that our study involves patients who are already in advanced disease and undergoing surgery anyway, so how many years they have smoked doesn't have to be so important at this point.

After a general look at the significance of variables, it is time to go into details.  It would be useful if we could explain to the patient why his chances of survival after surgery are as high as our model predicted and show him what could increase or decrease his chances.

For example, let's take a patient with the following variable values: 
- `date_start_treatment` 4.4
- `sex` K
- `years_smoking` 0
- `lung_cancer_in_family` No
- `symptoms` No
- `stadium_uicc` IIIB
- `age` 72
- `time_to_surgery` 0
- `log_tumor_volume` 5.8
Our most recent model indicated that the chances of survival after surgery for such a patient are $43\%.$
Here we have an explanation of this result by SHAP.
```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE, fig.cap="Shap method for choosen patient.'}
knitr::include_graphics('images/shap11_07.png')
```


We see that, according to the conclusions of the previous method, variables have the greatest impact: stadium_uicc and date_surgery. The SHAP method allows us to see whether individual variables have a negative or rather positive impact on predictions. Our patient's stadium_uicc is quite advanced, so it reduces the chances of survival after surgery quite drastically. The date_surgery variable, on the other hand, increases the probability, but we do not know whether a smaller value could further improve our prediction or the opposite.

To find out, we use another Ceteris Paribus method. First we'll look at the date variables.
If we look at the age variable, i.e. the age at the time of surgery, we can see that it behaves quite intuitively. The younger the patient, the better he is able to recover from surgery, so his chances for survival are higher.

```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE, fig.cap='Ceteris Paribus for age variable. '}
knitr::include_graphics('images/age_07.png')
```



The date_surgery variable is interesting. It indicates the number of years that have passed from surgery to the end of the study. It would seem that the earlier we do surgery, the better our prognosis will be. The charts show the opposite. This may be because if the surgery was done a long time ago, the patient has been getting older since then and has little chance of survival.
```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE, fig.cap='Ceteris Paribus for date_surgery variable. '}
knitr::include_graphics('images/date_birth_07_07.png')
```



Now let's look at the stadium. As mentioned earlier, the more advanced the stage of cancer the worse for the patient. Our patient's stadium is quite advanced, which has a big influence on the predictions, but if she had a more benign stage her chances of survival would increase strongly.

```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE, fig.cap='Ceteris Paribus for stadium_uicc variable. '}
knitr::include_graphics('images/uicc_07.png')
```


The variable log_tumor_volume indicates that if we cut out a smaller tumor, our postoperative survival increases. This conclusion coincides with our intuition, the removal of a smaller tumor is much safer, because perhaps with a larger one we could damage certain structures and thus increase mortality.
```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE, fig.cap='Ceteris Paribus for the volume of the tumor, but after transformation as a logarithm. '}
knitr::include_graphics('images/tumor_07.png')
```



 But using model explanations not only helps to explain the result of the prediction, it can also give a hint how to improve our model.
 
 When we built the model on all variables, the explanations allowed us to find correlations. Let's compare the CeterisParibus results for the full model and the deleted dependent variables. The image below shows the date_birth variable in its original version, i.e. as date.
 
```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE}
knitr::include_graphics('images/07_cp_3.png')
```
 In the picture above you can see that the influence of two correlated variables was distributed between them, but after leaving only one of these variables, the influence accumulated on it (picture below).
 
```{r, cache=FALSE, out.width="600", fig.align="center", echo=FALSE}
knitr::include_graphics('images/07_cp_4.png')
```

At this stage we can already conclude that the techniques of model explanations are not only useful at the end of our journey. They can give us tips on how to transform data or which variables should be deleted.
 

## Summary and conclusions 

- XAI methods legitimised employed approach of pruning the dataset.
- XAI methods yielded explainations consistent with biological intuintion, what builds up trust in the model.
- As variety of modelling and preprocessing approaches resulted in similar predicitive performance we conclude there is not much more to squeeze out of the dataset.

